{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a9b7c88",
   "metadata": {},
   "source": [
    "# ETL RAW -> SILVER | SINISTROS PRF\n",
    "\n",
    "## 1. Configuração e Importações\n",
    "\n",
    "Nesta etapa configuramos o ambiente do notebook, importamos bibliotecas e definimos os caminhos da camada **raw** e **silver**.  \n",
    "Também carregamos as variáveis do banco via arquivo `.env`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0bd2dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de 2 arquivos encontrados em raw: acidentes2024_todas_causas_tipos.csv, acidentes2025_todas_causas_tipos.csv\n",
      "DB host: 127.0.0.1\n",
      "DB port: 5433\n",
      "DB name: prf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 80)\n",
    "\n",
    "BASE_PATH = Path(os.getcwd()).parent.parent\n",
    "RAW_PATH = BASE_PATH / \"data_layer\" / \"raw\"\n",
    "SILVER_PATH = BASE_PATH / \"data_layer\" / \"silver\" / \"data\"\n",
    "SILVER_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_FILES = sorted([p for p in RAW_PATH.iterdir() if p.suffix.lower() == \".csv\"])\n",
    "\n",
    "print(f\"Total de {len(RAW_FILES)} arquivos encontrados em raw: {', '.join([p.name for p in RAW_FILES])}\")\n",
    "\n",
    "load_dotenv(BASE_PATH / \".env\")\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"host\": os.getenv(\"DB_HOST\"),\n",
    "    \"port\": os.getenv(\"DB_PORT\"),\n",
    "    \"database\": os.getenv(\"POSTGRES_DB\"),\n",
    "    \"user\": os.getenv(\"POSTGRES_USER\"),\n",
    "    \"password\": os.getenv(\"POSTGRES_PASSWORD\"),\n",
    "}\n",
    "\n",
    "print(\"DB host:\", DB_CONFIG[\"host\"])\n",
    "print(\"DB port:\", DB_CONFIG[\"port\"])\n",
    "print(\"DB name:\", DB_CONFIG[\"database\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10648602",
   "metadata": {},
   "source": [
    "## 2. Funções Auxiliares (Limpeza e Conversões)\n",
    "\n",
    "Aqui criamos funções reutilizáveis para padronizar texto, tratar valores nulos, converter tipos e validar campos importantes (UF, coordenadas, etc.).  \n",
    "Essas funções deixam o `transform` mais limpo e fácil de manter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd70f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL_LIKE = {\n",
    "    \"\", \" \", \"null\", \"none\", \"nan\", \"na\", \"n/a\",\n",
    "    \"(null)\", \"nonetype\", \"nan\", \"null\", \"n/a\"\n",
    "}\n",
    "\n",
    "UNKNOWN_LIKE = {\n",
    "    \"ignorado\",\n",
    "    \"nao informado\",\n",
    "    \"nao-informado\",\n",
    "    \"sem informacao\",\n",
    "    \"sem-informacao\",\n",
    "    \"desconhecido\",\n",
    "    \"0\",\n",
    "}\n",
    "\n",
    "VALID_UF = {\n",
    "    \"AC\",\"AL\",\"AP\",\"AM\",\"BA\",\"CE\",\"DF\",\"ES\",\"GO\",\"MA\",\"MT\",\"MS\",\"MG\",\n",
    "    \"PA\",\"PB\",\"PR\",\"PE\",\"PI\",\"RJ\",\"RN\",\"RS\",\"RO\",\"RR\",\"SC\",\"SP\",\"SE\",\"TO\"\n",
    "}\n",
    "\n",
    "SILVER_COLUMNS = [\n",
    "    \"ano_arquivo\", \"sinistro_id\", \"pessoa_id\", \"veiculo_id\",\n",
    "    \"data_hora\", \"dia_semana_num\", \"uf\", \"municipio\", \"delegacia\",\n",
    "    \"latitude\", \"longitude\", \"causa_acidente\", \"tipo_acidente\",\n",
    "    \"fase_dia\", \"sentido_via\", \"condicao_meteorologica\", \"tipo_pista\",\n",
    "    \"tracado_via\", \"caracteristicas_via\", \"tipo_envolvido\",\n",
    "    \"estado_fisico\", \"faixa_etaria\", \"sexo\", \"tipo_veiculo\",\n",
    "    \"faixa_idade_veiculo\",\n",
    "]\n",
    "\n",
    "def normalize_scalar(x) -> str | None:\n",
    "    if x is None or pd.isna(x):\n",
    "        return None\n",
    "    return (\n",
    "        unicodedata.normalize(\"NFKD\", str(x))\n",
    "        .encode(\"ascii\", \"ignore\")\n",
    "        .decode(\"ascii\")\n",
    "        .lower()\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_text(s: pd.Series) -> pd.Series:\n",
    "    if s is None:\n",
    "        return pd.Series(dtype=\"string\")\n",
    "\n",
    "    s = s.astype(\"string\")\n",
    "    s = s.map(lambda x: unicodedata.normalize(\"NFKC\", x) if pd.notna(x) else x)\n",
    "    s = s.str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "    normalized = s.map(normalize_scalar)\n",
    "    return s.mask(normalized.isin(NULL_LIKE), pd.NA)\n",
    "\n",
    "def to_int(s: pd.Series) -> pd.Series:\n",
    "    if s is None:\n",
    "        return pd.Series(dtype=\"Int64\")\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "\n",
    "def to_float(s: pd.Series) -> pd.Series:\n",
    "    if s is None:\n",
    "        return pd.Series(dtype=\"Float64\")\n",
    "    s = s.astype(\"string\").str.replace(\",\", \".\", regex=False)\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Float64\")\n",
    "\n",
    "\n",
    "def validate_coordinates(\n",
    "    lat: pd.Series,\n",
    "    lon: pd.Series\n",
    ") -> tuple[pd.Series, pd.Series]:\n",
    "    lat = lat.where(lat.isna() | lat.between(-90, 90), pd.NA)\n",
    "    lon = lon.where(lon.isna() | lon.between(-180, 180), pd.NA)\n",
    "    return lat.astype(\"Float64\"), lon.astype(\"Float64\")\n",
    "\n",
    "\n",
    "def validate_uf(s: pd.Series) -> pd.Series:\n",
    "    s = normalize_text(s).str.upper()\n",
    "    return s.where(s.isin(VALID_UF), pd.NA).astype(\"string\")\n",
    "\n",
    "\n",
    "def null_if_unknown(s: pd.Series) -> pd.Series:\n",
    "    s = normalize_text(s)\n",
    "\n",
    "    def resolve(v):\n",
    "        if v is None or pd.isna(v):\n",
    "            return pd.NA\n",
    "        return pd.NA if normalize_scalar(v) in UNKNOWN_LIKE else v\n",
    "\n",
    "    return s.map(resolve).astype(\"string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e628c107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_series(s: pd.Series) -> pd.Series:\n",
    "    return normalize_text(s).map(normalize_scalar)\n",
    "\n",
    "\n",
    "def parse_time(s: pd.Series) -> pd.Series:\n",
    "    s = normalize_text(s)\n",
    "    return (\n",
    "        pd.to_datetime(s, format=\"%H:%M:%S\", errors=\"coerce\")\n",
    "        .fillna(pd.to_datetime(s, format=\"%H:%M\", errors=\"coerce\"))).dt.time\n",
    "\n",
    "\n",
    "DAY_MAP = {\n",
    "    \"segunda-feira\": 0,\n",
    "    \"terca-feira\": 1,\n",
    "    \"terça-feira\": 1,\n",
    "    \"quarta-feira\": 2,\n",
    "    \"quinta-feira\": 3,\n",
    "    \"sexta-feira\": 4,\n",
    "    \"sabado\": 5,\n",
    "    \"sábado\": 5,\n",
    "    \"domingo\": 6,\n",
    "}\n",
    "\n",
    "def map_weekday(s: pd.Series) -> pd.Series:\n",
    "    return normalize_series(s).map(DAY_MAP).astype(\"Int64\")\n",
    "\n",
    "\n",
    "GENDER_MAP = {\n",
    "    \"masculino\": {\"m\", \"masc\", \"masculino\"},\n",
    "    \"feminino\": {\"f\", \"fem\", \"feminino\"},\n",
    "}\n",
    "\n",
    "def map_gender(s: pd.Series) -> pd.Series:\n",
    "    s = normalize_series(s)\n",
    "\n",
    "    def resolve(v):\n",
    "        for gender, values in GENDER_MAP.items():\n",
    "            if v in values:\n",
    "                return gender\n",
    "        return pd.NA\n",
    "\n",
    "    return s.map(resolve).astype(\"string\")\n",
    "\n",
    "\n",
    "\n",
    "PHYSICAL_STATE_RULES = [\n",
    "    (\"obito\", {\"obito\", \"morto\"}),\n",
    "    (\"grave\", {\"grave\"}),\n",
    "    (\"leve\", {\"leve\"}),\n",
    "    (\"ileso\", {\"ileso\", \"sem ferimentos\"}),\n",
    "]\n",
    "\n",
    "def map_physical_state(s: pd.Series) -> pd.Series:\n",
    "    s = normalize_series(s)\n",
    "\n",
    "    def resolve(v):\n",
    "        if v is None:\n",
    "            return pd.NA\n",
    "        for state, keywords in PHYSICAL_STATE_RULES:\n",
    "            if any(k in v for k in keywords):\n",
    "                return state\n",
    "        return pd.NA\n",
    "\n",
    "    return s.map(resolve).astype(\"string\")\n",
    "\n",
    "\n",
    "LAND_USE_MAP = {\n",
    "    \"sim\": \"urbano\",\n",
    "    \"nao\": \"rural\",\n",
    "}\n",
    "\n",
    "def map_land_use(s: pd.Series) -> pd.Series:\n",
    "    return normalize_series(s).map(LAND_USE_MAP).astype(\"string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e63ff0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Idade para faixas.\n",
    "def age_bucket(age_s: pd.Series) -> pd.Series:\n",
    "    if age_s is None:\n",
    "        return pd.Series([], dtype=\"string\")\n",
    "    age = pd.to_numeric(age_s, errors=\"coerce\")\n",
    "    age = age.mask((age <= 0) | (age > 120), np.nan)\n",
    "\n",
    "    bins = [-0.1, 9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 10_000]\n",
    "    labels = [\"0-9\",\"10-19\",\"20-29\",\"30-39\",\"40-49\",\"50-59\",\"60-69\",\"70-79\",\"80-89\",\"90-99\",\"100+\"]\n",
    "\n",
    "    return pd.cut(age, bins=bins, labels=labels).astype(\"string\")\n",
    "\n",
    "# Ano fabricação para idade do veículo e depois para faixas\n",
    "def vehicle_age_bucket(year_fab_s: pd.Series, year_ref_s: pd.Series) -> pd.Series:\n",
    "    if year_fab_s is None or year_ref_s is None:\n",
    "        return pd.Series([], dtype=\"string\")\n",
    "    year_fab = pd.to_numeric(year_fab_s, errors=\"coerce\")\n",
    "    year_ref = pd.to_numeric(year_ref_s, errors=\"coerce\")\n",
    "\n",
    "    age = (year_ref - year_fab).mask(lambda x: (x < 0) | (x > 120), np.nan)\n",
    "\n",
    "    bins = [-0.1, 4, 9, 14, 19, 29, 120]\n",
    "    labels = [\"0-4\",\"5-9\",\"10-14\",\"15-19\",\"20-29\",\"30+\"]\n",
    "\n",
    "    return pd.cut(age, bins=bins, labels=labels).astype(\"string\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d63a2a",
   "metadata": {},
   "source": [
    "## 3. Funções de Banco (Load no PostgreSQL)\n",
    "\n",
    "Nesta etapa criamos as funções que conectam no PostgreSQL e fazem o carregamento na tabela `silver.sinistros`.  \n",
    "O carregamento usa `execute_values` para inserir em lote e suportar os modos **truncate** (recarregar tudo) ou **upsert** (atualizar quando repetir PK).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a03cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_SCHEMA = \"silver\"\n",
    "TABLE_NAME = \"sinistros\"\n",
    "TABLE_FULL_NAME = f\"{DB_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "def get_conn(db_config: dict):\n",
    "    return psycopg2.connect(\n",
    "        host=db_config[\"host\"],\n",
    "        port=db_config[\"port\"],\n",
    "        dbname=db_config[\"database\"],\n",
    "        user=db_config[\"user\"],\n",
    "        password=db_config[\"password\"],\n",
    "    )\n",
    "\n",
    "def get_row_count(cur) -> int:\n",
    "    cur.execute(f\"SELECT COUNT(*) FROM {TABLE_FULL_NAME};\")\n",
    "    return cur.fetchone()[0]\n",
    "\n",
    "def ensure_schema_and_table(cur):\n",
    "    ddl_path = BASE_PATH / \"data_layer\" / \"silver\" / \"ddl.sql\"\n",
    "    \n",
    "    if not ddl_path.exists():\n",
    "        print(f\"Arquivo DDL não encontrado: {ddl_path}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        ddl = ddl_path.read_text(encoding=\"utf-8\")\n",
    "        cur.execute(ddl)\n",
    "        print(\"Estrutura do banco criada com sucesso\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao executar DDL: {e}\")\n",
    "\n",
    "def load_to_postgres(df: pd.DataFrame, db_config: dict, mode: str = \"truncate\"):\n",
    "    df_load = df[SILVER_COLUMNS].copy()\n",
    "    df_load = df_load.astype(object).where(pd.notna(df_load), None)\n",
    "    records = [tuple(row) for row in df_load.itertuples(index=False, name=None)]\n",
    "\n",
    "    if not records:\n",
    "        print(\"Nenhum registro para carregar (DataFrame vazio)\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total de linhas a serem carregadas: {len(records):,}\")\n",
    "    \n",
    "    insert_sql = f\"\"\"\n",
    "        INSERT INTO {TABLE_FULL_NAME} ({\",\".join(SILVER_COLUMNS)})\n",
    "        VALUES %s\n",
    "    \"\"\"\n",
    "\n",
    "    conn = get_conn(db_config)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        ensure_schema_and_table(cur)\n",
    "        conn.commit()\n",
    "        try:\n",
    "            count_before = get_row_count(cur)\n",
    "            print(f\"Registros no banco antes da carga: {count_before:,}\")\n",
    "        except:\n",
    "            count_before = 0\n",
    "            print(\"Tabela ainda não existe ou está vazia\")\n",
    "\n",
    "        # Truncar se necessário\n",
    "        if mode == \"truncate\":\n",
    "            cur.execute(f\"TRUNCATE TABLE {TABLE_FULL_NAME};\")\n",
    "            conn.commit()\n",
    "        \n",
    "        print(\"⬆️  Inserindo dados...\")\n",
    "        execute_values(cur, insert_sql, records, page_size=5000)\n",
    "        conn.commit()\n",
    "        \n",
    "        count_after = get_row_count(cur)\n",
    "        print(f\"Carga concluída com sucesso!\")\n",
    "        print(f\"Registros no banco depois da carga: {count_after:,}\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Erro durante a carga: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        print(f\"{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f32da",
   "metadata": {},
   "source": [
    "## 4. Carregando os Dados Raw\n",
    "\n",
    "Agora carregamos todos os arquivos CSV da pasta `raw` e unimos em um único DataFrame.  \n",
    "Também exibimos o tamanho final para termos noção do volume de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "877943a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carregando dados Raw...\n",
      "Carregado: 1,131,595 linhas x 38 colunas\n",
      "Colunas: ['id', 'pesid', 'data_inversa', 'dia_semana', 'horario', 'uf', 'br', 'km', 'municipio', 'causa_principal', 'causa_acidente', 'ordem_tipo_acidente', 'tipo_acidente', 'classificacao_acidente', 'fase_dia', 'sentido_via', 'condicao_metereologica', 'tipo_pista', 'tracado_via', 'uso_solo', 'id_veiculo', 'tipo_veiculo', 'marca', 'ano_fabricacao_veiculo', 'tipo_envolvido', 'estado_fisico', 'idade', 'sexo', 'ilesos', 'feridos_leves', 'feridos_graves', 'mortos', 'latitude', 'longitude', 'regional', 'delegacia', 'uop', '__source_file']\n"
     ]
    }
   ],
   "source": [
    "# Carrega e junta os CSVs raw\n",
    "\n",
    "def load_raw_csvs(csv_paths: list[Path]) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    for p in csv_paths:\n",
    "        df = pd.read_csv(\n",
    "            p,\n",
    "            sep=\";\",\n",
    "            encoding=\"ISO-8859-1\",\n",
    "            low_memory=False,\n",
    "            dtype=str,\n",
    "        )\n",
    "        df[\"__source_file\"] = p.name\n",
    "        dfs.append(df)\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "print(\"\\nCarregando dados Raw...\")\n",
    "df_raw = load_raw_csvs(RAW_FILES)\n",
    "\n",
    "print(f\"Carregado: {df_raw.shape[0]:,} linhas x {df_raw.shape[1]:,} colunas\")\n",
    "print(\"Colunas:\", list(df_raw.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9172ae5",
   "metadata": {},
   "source": [
    "## 5. Transformação (RAW -> SILVER)\n",
    "\n",
    "Nesta etapa aplicamos as regras de limpeza e padronização:\n",
    "\n",
    "- Normalização de strings e tratamento de valores inválidos  \n",
    "- Conversão de IDs e remoção de registros sem chave primária  \n",
    "- Criação de `data_hora` e `ano_arquivo`  \n",
    "- Validação de UF e coordenadas  \n",
    "- Criação de features derivadas (faixa etária, idade do veículo, etc.)  \n",
    "- Deduplicação pela chave `(sinistro_id, pessoa_id)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ad581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INICIANDO TRANSFORM (RAW -> SILVER)\n",
      "Shape inicial: (1131595, 38)\n",
      "Normalizando texto...\n",
      "Convertendo IDs...\n",
      "Convertendo data e horário...\n",
      "Criando data_hora...\n",
      "Convertendo latitude/longitude...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def to_silver(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"\\nINICIANDO TRANSFORM (RAW -> SILVER)\")\n",
    "    print(f\"Shape inicial: {df.shape}\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalização geral\n",
    "    print(\"Normalizando texto...\")\n",
    "    for col in df.columns:\n",
    "        df[col] = normalize_text(df[col])\n",
    "\n",
    "    # Corrigir typo de coluna\n",
    "    if \"condicao_metereologica\" in df.columns:\n",
    "        df = df.rename(columns={\"condicao_metereologica\": \"condicao_meteorologica\"})\n",
    "\n",
    "    df[\"condicao_meteorologica\"] = null_if_unknown(df.get(\"condicao_meteorologica\"))\n",
    "\n",
    "    # IDs\n",
    "    print(\"Convertendo IDs...\")\n",
    "    df[\"sinistro_id\"] = to_int(df[\"id\"]) if \"id\" in df.columns else pd.NA\n",
    "    df[\"pessoa_id\"] = to_int(df[\"pesid\"]) if \"pesid\" in df.columns else pd.NA\n",
    "    df[\"veiculo_id\"] = to_int(df[\"id_veiculo\"]) if \"id_veiculo\" in df.columns else pd.NA\n",
    "    df[\"sinistro_id\"] = df[\"sinistro_id\"].where(df[\"sinistro_id\"].isna() | (df[\"sinistro_id\"] > 0), pd.NA)\n",
    "    df[\"pessoa_id\"]   = df[\"pessoa_id\"].where(df[\"pessoa_id\"].isna() | (df[\"pessoa_id\"] > 0), pd.NA)\n",
    "    df[\"veiculo_id\"]  = df[\"veiculo_id\"].where(df[\"veiculo_id\"].isna() | (df[\"veiculo_id\"] > 0), pd.NA)\n",
    "\n",
    "\n",
    "    # Data/hora\n",
    "    print(\"Convertendo data e horário...\")\n",
    "    df[\"date_dt\"] = pd.to_datetime(df.get(\"data_inversa\"), format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    df[\"time_dt\"] = parse_time(df.get(\"horario\"))\n",
    "\n",
    "    print(\"Criando data_hora...\")\n",
    "    time_txt = df[\"time_dt\"].astype(\"string\").fillna(\"00:00:00\")\n",
    "    df[\"data_hora\"] = pd.to_datetime(df[\"date_dt\"].astype(\"string\") + \" \" + time_txt, errors=\"coerce\")\n",
    "\n",
    "    df[\"ano_arquivo\"] = df[\"data_hora\"].dt.year.astype(\"Int64\")\n",
    "    df[\"dia_semana_num\"] = map_weekday(df.get(\"dia_semana\"))\n",
    "\n",
    "    # Coordenadas\n",
    "    print(\"Convertendo latitude/longitude...\")\n",
    "    df[\"latitude\"] = to_float(df.get(\"latitude\"))\n",
    "    df[\"longitude\"] = to_float(df.get(\"longitude\"))\n",
    "    df[\"latitude\"], df[\"longitude\"] = validate_coordinates(df[\"latitude\"], df[\"longitude\"])\n",
    "\n",
    "    # Campos derivados\n",
    "    df[\"caracteristicas_via\"] = map_land_use(df.get(\"uso_solo\"))\n",
    "    df[\"sexo_condutor\"] = map_gender(df.get(\"sexo\"))\n",
    "    df[\"estado_fisico\"] = map_physical_state(df.get(\"estado_fisico\"))\n",
    "    df[\"faixa_etaria\"] = age_bucket(df.get(\"idade\"))\n",
    "    df[\"faixa_idade_veiculo\"] = vehicle_age_bucket(df.get(\"ano_fabricacao_veiculo\"), df[\"ano_arquivo\"])\n",
    "\n",
    "    # UF + município\n",
    "    df[\"uf\"] = validate_uf(df.get(\"uf\"))\n",
    "    df[\"municipio\"] = df.get(\"municipio\").str.upper()\n",
    "\n",
    "    # Filtro de ano\n",
    "    print(\"Aplicando filtro ano_arquivo (2024/2025)...\")\n",
    "    before = len(df)\n",
    "    df = df[df[\"ano_arquivo\"].isin([2024, 2025])].copy()\n",
    "    print(f\"   Linhas removidas pelo filtro: {before - len(df):,}\")\n",
    "\n",
    "    # Garantir colunas finais\n",
    "    print(\"Garantindo colunas do contrato...\")\n",
    "    for col in SILVER_COLUMNS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    \n",
    "    before_pk = len(df)\n",
    "    df = df[df[\"sinistro_id\"].notna() & df[\"pessoa_id\"].notna()].copy()\n",
    "    print(f\"   Linhas removidas por PK inválida: {before_pk - len(df):,}\")\n",
    "\n",
    "\n",
    "    # Deduplicação por PK\n",
    "    print(\"Removendo duplicatas por (sinistro_id, pessoa_id)...\")\n",
    "    before = len(df)\n",
    "    df[\"__completeness\"] = df[SILVER_COLUMNS].notna().sum(axis=1)\n",
    "\n",
    "    df = df.sort_values(\n",
    "        [\"sinistro_id\", \"pessoa_id\", \"__completeness\", \"data_hora\"],\n",
    "        ascending=[True, True, False, False],\n",
    "        na_position=\"last\",\n",
    "    )\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"sinistro_id\", \"pessoa_id\"], keep=\"first\").drop(columns=\"__completeness\")\n",
    "    print(f\"   Duplicatas removidas: {before - len(df):,}\")\n",
    "\n",
    "    df_silver = df[SILVER_COLUMNS].copy()\n",
    "    print(f\"Shape final (silver): {df_silver.shape}\")\n",
    "    return df_silver\n",
    "\n",
    "df_silver = to_silver(df_raw)\n",
    "df_silver.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375570c8",
   "metadata": {},
   "source": [
    "## 6. Carga no PostgreSQL (silver.sinistros)\n",
    "\n",
    "Por fim, carregamos o dataset processado no banco PostgreSQL.  \n",
    "O modo `truncate` apaga tudo e recarrega do zero.  \n",
    "Depois validamos conferindo o total de registros na tabela.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023d47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CARREGANDO NO POSTGRES (silver.sinistros)...\n",
      "Total de linhas a serem carregadas: 339,183\n",
      "Estrutura do banco criada com sucesso\n",
      "Registros no banco antes da carga: 0\n",
      "⬆️  Inserindo dados...\n",
      "Carga concluída com sucesso!\n",
      "Registros no banco depois da carga: 339,183\n",
      "================================================================================\n",
      "\n",
      "Total no banco: 339,183\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCARREGANDO NO POSTGRES (silver.sinistros)...\")\n",
    "\n",
    "\n",
    "# Mode:\"truncate\" = limpa e recarrega tudo, \"upsert\"   = insere/atualiza se repetir PK\n",
    "load_to_postgres(df_silver, DB_CONFIG, mode=\"truncate\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
