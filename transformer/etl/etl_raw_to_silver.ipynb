{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a9b7c88",
   "metadata": {},
   "source": [
    "# ETL RAW -> SILVER | SINISTROS PRF\n",
    "\n",
    "## 1. Configuração e Importações\n",
    "\n",
    "Nesta etapa configuramos o ambiente do notebook, importamos bibliotecas e definimos os caminhos da camada **raw** e **silver**.  \n",
    "Também carregamos as variáveis do banco via arquivo `.env`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0bd2dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de 2 arquivos encontrados em raw: acidentes2024_todas_causas_tipos.csv, acidentes2025_todas_causas_tipos.csv\n",
      "DB host: localhost\n",
      "DB port: 5432\n",
      "DB name: prf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 80)\n",
    "\n",
    "BASE_PATH = Path(os.getcwd()).parent.parent\n",
    "RAW_PATH = BASE_PATH / \"data_layer\" / \"raw\"\n",
    "SILVER_PATH = BASE_PATH / \"data_layer\" / \"silver\" / \"data\"\n",
    "SILVER_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_FILES = sorted([p for p in RAW_PATH.iterdir() if p.suffix.lower() == \".csv\"])\n",
    "\n",
    "print(f\"Total de {len(RAW_FILES)} arquivos encontrados em raw: {', '.join([p.name for p in RAW_FILES])}\")\n",
    "\n",
    "load_dotenv(BASE_PATH / \".env\")\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"host\": os.getenv(\"DB_HOST\"),\n",
    "    \"port\": os.getenv(\"DB_PORT\"),\n",
    "    \"database\": os.getenv(\"POSTGRES_DB\"),\n",
    "    \"user\": os.getenv(\"POSTGRES_USER\"),\n",
    "    \"password\": os.getenv(\"POSTGRES_PASSWORD\"),\n",
    "}\n",
    "\n",
    "print(\"DB host:\", DB_CONFIG[\"host\"])\n",
    "print(\"DB port:\", DB_CONFIG[\"port\"])\n",
    "print(\"DB name:\", DB_CONFIG[\"database\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10648602",
   "metadata": {},
   "source": [
    "## 2. Funções Auxiliares (Limpeza e Conversões)\n",
    "\n",
    "Aqui criamos funções reutilizáveis para padronizar texto, tratar valores nulos, converter tipos e validar campos importantes (UF, coordenadas, etc.).  \n",
    "Essas funções deixam o `transform` mais limpo e fácil de manter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL_LIKE = {\n",
    "    \"\", \" \", \"null\", \"none\", \"nan\", \"na\", \"n/a\",\n",
    "    \"(null)\", \"nonetype\", \"nan\", \"null\", \"n/a\"\n",
    "}\n",
    "\n",
    "UNKNOWN_LIKE = {\n",
    "    \"ignorado\",\n",
    "    \"nao informado\",\n",
    "    \"nao-informado\",\n",
    "    \"sem informacao\",\n",
    "    \"sem-informacao\",\n",
    "    \"desconhecido\",\n",
    "    \"0\",\n",
    "}\n",
    "\n",
    "VALID_UF = {\n",
    "    \"AC\",\"AL\",\"AP\",\"AM\",\"BA\",\"CE\",\"DF\",\"ES\",\"GO\",\"MA\",\"MT\",\"MS\",\"MG\",\n",
    "    \"PA\",\"PB\",\"PR\",\"PE\",\"PI\",\"RJ\",\"RN\",\"RS\",\"RO\",\"RR\",\"SC\",\"SP\",\"SE\",\"TO\"\n",
    "}\n",
    "\n",
    "SILVER_COLUMNS = [\n",
    "    \"ano_arquivo\", \"sinistro_id\", \"pessoa_id\", \"veiculo_id\",\n",
    "    \"data_hora\", \"dia_semana_num\", \"uf\", \"municipio\", \"delegacia\",\n",
    "    \"latitude\", \"longitude\", \"causa_acidente\", \"tipo_acidente\",\n",
    "    \"fase_dia\", \"sentido_via\", \"condicao_meteorologica\", \"tipo_pista\",\n",
    "    \"tracado_via\", \"caracteristicas_via\", \"tipo_envolvido\",\n",
    "    \"estado_fisico\", \"faixa_etaria\", \"sexo\", \"tipo_veiculo\",\n",
    "    \"faixa_idade_veiculo\",\n",
    "]\n",
    "\n",
    "def normalize_scalar(x) -> str | None:\n",
    "    if x is None or pd.isna(x):\n",
    "        return None\n",
    "    return (\n",
    "        unicodedata.normalize(\"NFKD\", str(x))\n",
    "        .encode(\"ascii\", \"ignore\")\n",
    "        .decode(\"ascii\")\n",
    "        .lower()\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_text(s: pd.Series) -> pd.Series:\n",
    "    if s is None:\n",
    "        return pd.Series(dtype=\"string\")\n",
    "\n",
    "    s = s.astype(\"string\")\n",
    "    s = s.map(lambda x: unicodedata.normalize(\"NFKC\", x) if pd.notna(x) else x)\n",
    "    s = s.str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "    normalized = s.map(normalize_scalar)\n",
    "    return s.mask(normalized.isin(NULL_LIKE), pd.NA)\n",
    "\n",
    "def to_int(s: pd.Series) -> pd.Series:\n",
    "    if s is None:\n",
    "        return pd.Series(dtype=\"Int64\")\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "\n",
    "def to_float(s: pd.Series) -> pd.Series:\n",
    "    if s is None:\n",
    "        return pd.Series(dtype=\"Float64\")\n",
    "    s = s.astype(\"string\").str.replace(\",\", \".\", regex=False)\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Float64\")\n",
    "\n",
    "\n",
    "def validate_coordinates(\n",
    "    lat: pd.Series,\n",
    "    lon: pd.Series\n",
    ") -> tuple[pd.Series, pd.Series]:\n",
    "    lat = lat.where(lat.isna() | lat.between(-90, 90), pd.NA)\n",
    "    lon = lon.where(lon.isna() | lon.between(-180, 180), pd.NA)\n",
    "    return lat.astype(\"Float64\"), lon.astype(\"Float64\")\n",
    "\n",
    "\n",
    "def validate_uf(s: pd.Series) -> pd.Series:\n",
    "    s = normalize_text(s).str.upper()\n",
    "    return s.where(s.isin(VALID_UF), pd.NA).astype(\"string\")\n",
    "\n",
    "\n",
    "def null_if_unknown(s: pd.Series) -> pd.Series:\n",
    "    s = normalize_text(s)\n",
    "\n",
    "    def resolve(v):\n",
    "        if v is None or pd.isna(v):\n",
    "            return pd.NA\n",
    "        return pd.NA if normalize_scalar(v) in UNKNOWN_LIKE else v\n",
    "\n",
    "    return s.map(resolve).astype(\"string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e628c107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_series(s: pd.Series) -> pd.Series:\n",
    "    return normalize_text(s).map(normalize_scalar)\n",
    "\n",
    "\n",
    "def parse_time(s: pd.Series) -> pd.Series:\n",
    "    s = normalize_text(s)\n",
    "    return (\n",
    "        pd.to_datetime(s, format=\"%H:%M:%S\", errors=\"coerce\")\n",
    "        .fillna(pd.to_datetime(s, format=\"%H:%M\", errors=\"coerce\"))).dt.time\n",
    "\n",
    "\n",
    "DAY_MAP = {\n",
    "    \"segunda-feira\": 0,\n",
    "    \"terca-feira\": 1,\n",
    "    \"terça-feira\": 1,\n",
    "    \"quarta-feira\": 2,\n",
    "    \"quinta-feira\": 3,\n",
    "    \"sexta-feira\": 4,\n",
    "    \"sabado\": 5,\n",
    "    \"sábado\": 5,\n",
    "    \"domingo\": 6,\n",
    "}\n",
    "\n",
    "def map_weekday(s: pd.Series) -> pd.Series:\n",
    "    return normalize_series(s).map(DAY_MAP).astype(\"Int64\")\n",
    "\n",
    "\n",
    "GENDER_MAP = {\n",
    "    \"masculino\": {\"m\", \"masc\", \"masculino\"},\n",
    "    \"feminino\": {\"f\", \"fem\", \"feminino\"},\n",
    "}\n",
    "\n",
    "def map_gender(s: pd.Series) -> pd.Series:\n",
    "    s = normalize_series(s)\n",
    "\n",
    "    def resolve(v):\n",
    "        for gender, values in GENDER_MAP.items():\n",
    "            if v in values:\n",
    "                return gender\n",
    "        return pd.NA\n",
    "\n",
    "    return s.map(resolve).astype(\"string\")\n",
    "\n",
    "\n",
    "\n",
    "PHYSICAL_STATE_RULES = [\n",
    "    (\"obito\", {\"obito\", \"morto\"}),\n",
    "    (\"grave\", {\"grave\"}),\n",
    "    (\"leve\", {\"leve\"}),\n",
    "    (\"ileso\", {\"ileso\", \"sem ferimentos\"}),\n",
    "]\n",
    "\n",
    "def map_physical_state(s: pd.Series) -> pd.Series:\n",
    "    s = normalize_series(s)\n",
    "\n",
    "    def resolve(v):\n",
    "        if v is None:\n",
    "            return pd.NA\n",
    "        for state, keywords in PHYSICAL_STATE_RULES:\n",
    "            if any(k in v for k in keywords):\n",
    "                return state\n",
    "        return pd.NA\n",
    "\n",
    "    return s.map(resolve).astype(\"string\")\n",
    "\n",
    "\n",
    "LAND_USE_MAP = {\n",
    "    \"sim\": \"urbano\",\n",
    "    \"nao\": \"rural\",\n",
    "}\n",
    "\n",
    "def map_land_use(s: pd.Series) -> pd.Series:\n",
    "    return normalize_series(s).map(LAND_USE_MAP).astype(\"string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ff0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Idade para faixas.\n",
    "def age_bucket(age_s: pd.Series) -> pd.Series:\n",
    "    if age_s is None:\n",
    "        return pd.Series([], dtype=\"string\")\n",
    "    age = pd.to_numeric(age_s, errors=\"coerce\")\n",
    "    age = age.mask((age <= 0) | (age > 120), np.nan)\n",
    "\n",
    "    bins = [-0.1, 9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 10_000]\n",
    "    labels = [\"0-9\",\"10-19\",\"20-29\",\"30-39\",\"40-49\",\"50-59\",\"60-69\",\"70-79\",\"80-89\",\"90-99\",\"100+\"]\n",
    "\n",
    "    return pd.cut(age, bins=bins, labels=labels).astype(\"string\")\n",
    "\n",
    "# Ano fabricação para idade do veículo e depois para faixas\n",
    "def vehicle_age_bucket(year_fab_s: pd.Series, year_ref_s: pd.Series) -> pd.Series:\n",
    "    if year_fab_s is None or year_ref_s is None:\n",
    "        return pd.Series([], dtype=\"string\")\n",
    "    year_fab = pd.to_numeric(year_fab_s, errors=\"coerce\")\n",
    "    year_ref = pd.to_numeric(year_ref_s, errors=\"coerce\")\n",
    "\n",
    "    age = (year_ref - year_fab).mask(lambda x: (x < 0) | (x > 120), np.nan)\n",
    "\n",
    "    bins = [-0.1, 4, 9, 14, 19, 29, 120]\n",
    "    labels = [\"0-4\",\"5-9\",\"10-14\",\"15-19\",\"20-29\",\"30+\"]\n",
    "\n",
    "    return pd.cut(age, bins=bins, labels=labels).astype(\"string\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d63a2a",
   "metadata": {},
   "source": [
    "## 3. Funções de Banco (Load no PostgreSQL)\n",
    "\n",
    "Nesta etapa criamos as funções que conectam no PostgreSQL e fazem o carregamento na tabela `silver.sinistros`.  \n",
    "O carregamento usa `execute_values` para inserir em lote e suportar os modos **truncate** (recarregar tudo) ou **upsert** (atualizar quando repetir PK).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a03cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_SCHEMA = \"silver\"\n",
    "TABLE_NAME = \"sinistros\"\n",
    "TABLE_FULL_NAME = f\"{DB_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "def get_conn(db_config: dict):\n",
    "    return psycopg2.connect(\n",
    "        host=db_config[\"host\"],\n",
    "        port=db_config[\"port\"],\n",
    "        dbname=db_config[\"database\"],\n",
    "        user=db_config[\"user\"],\n",
    "        password=db_config[\"password\"],\n",
    "    )\n",
    "\n",
    "def get_row_count(cur) -> int:\n",
    "    cur.execute(f\"SELECT COUNT(*) FROM {TABLE_FULL_NAME};\")\n",
    "    return cur.fetchone()[0]\n",
    "\n",
    "def ensure_schema_and_table(cur):\n",
    "    ddl_path = BASE_PATH / \"data_layer\" / \"silver\" / \"ddl.sql\"\n",
    "    \n",
    "    if not ddl_path.exists():\n",
    "        print(f\"Arquivo DDL não encontrado: {ddl_path}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        ddl = ddl_path.read_text(encoding=\"utf-8\")\n",
    "        cur.execute(ddl)\n",
    "        print(\"Estrutura do banco criada com sucesso\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao executar DDL: {e}\")\n",
    "\n",
    "def load_to_postgres(df: pd.DataFrame, db_config: dict, mode: str = \"truncate\"):\n",
    "    df_load = df[SILVER_COLUMNS].copy()\n",
    "    df_load = df_load.astype(object).where(pd.notna(df_load), None)\n",
    "    records = [tuple(row) for row in df_load.itertuples(index=False, name=None)]\n",
    "\n",
    "    if not records:\n",
    "        print(\"Nenhum registro para carregar (DataFrame vazio)\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total de linhas a serem carregadas: {len(records):,}\")\n",
    "    \n",
    "    insert_sql = f\"\"\"\n",
    "        INSERT INTO {TABLE_FULL_NAME} ({\",\".join(SILVER_COLUMNS)})\n",
    "        VALUES %s\n",
    "    \"\"\"\n",
    "\n",
    "    conn = get_conn(db_config)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        ensure_schema_and_table(cur)\n",
    "        conn.commit()\n",
    "        try:\n",
    "            count_before = get_row_count(cur)\n",
    "            print(f\"Registros no banco antes da carga: {count_before:,}\")\n",
    "        except:\n",
    "            count_before = 0\n",
    "            print(\"Tabela ainda não existe ou está vazia\")\n",
    "\n",
    "        # Truncar se necessário\n",
    "        if mode == \"truncate\":\n",
    "            cur.execute(f\"TRUNCATE TABLE {TABLE_FULL_NAME};\")\n",
    "            conn.commit()\n",
    "        \n",
    "        print(\"⬆️  Inserindo dados...\")\n",
    "        execute_values(cur, insert_sql, records, page_size=5000)\n",
    "        conn.commit()\n",
    "        \n",
    "        count_after = get_row_count(cur)\n",
    "        print(f\"Carga concluída com sucesso!\")\n",
    "        print(f\"Registros no banco depois da carga: {count_after:,}\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Erro durante a carga: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        print(f\"{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f32da",
   "metadata": {},
   "source": [
    "## 4. Carregando os Dados Raw\n",
    "\n",
    "Agora carregamos todos os arquivos CSV da pasta `raw` e unimos em um único DataFrame.  \n",
    "Também exibimos o tamanho final para termos noção do volume de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "877943a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carregando dados Raw...\n",
      "Carregado: 1,131,595 linhas x 38 colunas\n",
      "Colunas: ['id', 'pesid', 'data_inversa', 'dia_semana', 'horario', 'uf', 'br', 'km', 'municipio', 'causa_principal', 'causa_acidente', 'ordem_tipo_acidente', 'tipo_acidente', 'classificacao_acidente', 'fase_dia', 'sentido_via', 'condicao_metereologica', 'tipo_pista', 'tracado_via', 'uso_solo', 'id_veiculo', 'tipo_veiculo', 'marca', 'ano_fabricacao_veiculo', 'tipo_envolvido', 'estado_fisico', 'idade', 'sexo', 'ilesos', 'feridos_leves', 'feridos_graves', 'mortos', 'latitude', 'longitude', 'regional', 'delegacia', 'uop', '__source_file']\n"
     ]
    }
   ],
   "source": [
    "# Carrega e junta os CSVs raw\n",
    "\n",
    "def load_raw_csvs(csv_paths: list[Path]) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    for p in csv_paths:\n",
    "        df = pd.read_csv(\n",
    "            p,\n",
    "            sep=\";\",\n",
    "            encoding=\"ISO-8859-1\",\n",
    "            low_memory=False,\n",
    "            dtype=str,\n",
    "        )\n",
    "        df[\"__source_file\"] = p.name\n",
    "        dfs.append(df)\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "print(\"\\nCarregando dados Raw...\")\n",
    "df_raw = load_raw_csvs(RAW_FILES)\n",
    "\n",
    "print(f\"Carregado: {df_raw.shape[0]:,} linhas x {df_raw.shape[1]:,} colunas\")\n",
    "print(\"Colunas:\", list(df_raw.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9172ae5",
   "metadata": {},
   "source": [
    "## 5. Transformação (RAW -> SILVER)\n",
    "\n",
    "Nesta etapa aplicamos as regras de limpeza e padronização:\n",
    "\n",
    "- Normalização de strings e tratamento de valores inválidos  \n",
    "- Conversão de IDs e remoção de registros sem chave primária  \n",
    "- Criação de `data_hora` e `ano_arquivo`  \n",
    "- Validação de UF e coordenadas  \n",
    "- Criação de features derivadas (faixa etária, idade do veículo, etc.)  \n",
    "- Deduplicação pela chave `(sinistro_id, pessoa_id)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f2ad581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INICIANDO TRANSFORM (RAW -> SILVER)\n",
      "Shape inicial: (1131595, 38)\n",
      "Normalizando texto...\n",
      "Convertendo IDs...\n",
      "Convertendo data e horário...\n",
      "Criando data_hora...\n",
      "Convertendo latitude/longitude...\n",
      "Aplicando filtro ano_arquivo (2024/2025)...\n",
      "   Linhas removidas pelo filtro: 0\n",
      "Garantindo colunas do contrato...\n",
      "   Linhas removidas por PK inválida: 110,106\n",
      "Removendo duplicatas por (sinistro_id, pessoa_id)...\n",
      "   Duplicatas removidas: 682,306\n",
      "Shape final (silver): (339183, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ano_arquivo</th>\n",
       "      <th>sinistro_id</th>\n",
       "      <th>pessoa_id</th>\n",
       "      <th>veiculo_id</th>\n",
       "      <th>data_hora</th>\n",
       "      <th>dia_semana_num</th>\n",
       "      <th>uf</th>\n",
       "      <th>municipio</th>\n",
       "      <th>delegacia</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>causa_acidente</th>\n",
       "      <th>tipo_acidente</th>\n",
       "      <th>fase_dia</th>\n",
       "      <th>sentido_via</th>\n",
       "      <th>condicao_meteorologica</th>\n",
       "      <th>tipo_pista</th>\n",
       "      <th>tracado_via</th>\n",
       "      <th>caracteristicas_via</th>\n",
       "      <th>tipo_envolvido</th>\n",
       "      <th>estado_fisico</th>\n",
       "      <th>faixa_etaria</th>\n",
       "      <th>sexo</th>\n",
       "      <th>tipo_veiculo</th>\n",
       "      <th>faixa_idade_veiculo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>571772</td>\n",
       "      <td>1268971</td>\n",
       "      <td>1018215</td>\n",
       "      <td>2024-01-01 00:05:00</td>\n",
       "      <td>0</td>\n",
       "      <td>RJ</td>\n",
       "      <td>TANGUA</td>\n",
       "      <td>DEL02-RJ</td>\n",
       "      <td>-22.72936</td>\n",
       "      <td>-42.701125</td>\n",
       "      <td>Reação tardia ou ineficiente do condutor</td>\n",
       "      <td>Colisão com objeto</td>\n",
       "      <td>Plena Noite</td>\n",
       "      <td>Decrescente</td>\n",
       "      <td>Céu Claro</td>\n",
       "      <td>Dupla</td>\n",
       "      <td>Reta</td>\n",
       "      <td>urbano</td>\n",
       "      <td>Condutor</td>\n",
       "      <td>obito</td>\n",
       "      <td>20-29</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>Motocicleta</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>571774</td>\n",
       "      <td>1268985</td>\n",
       "      <td>1018226</td>\n",
       "      <td>2024-01-01 00:05:00</td>\n",
       "      <td>0</td>\n",
       "      <td>GO</td>\n",
       "      <td>ANAPOLIS</td>\n",
       "      <td>DEL02-GO</td>\n",
       "      <td>-16.229185</td>\n",
       "      <td>-49.009797</td>\n",
       "      <td>Animais na Pista</td>\n",
       "      <td>Colisão com objeto</td>\n",
       "      <td>Plena Noite</td>\n",
       "      <td>Decrescente</td>\n",
       "      <td>Céu Claro</td>\n",
       "      <td>Dupla</td>\n",
       "      <td>Reta</td>\n",
       "      <td>rural</td>\n",
       "      <td>Condutor</td>\n",
       "      <td>ileso</td>\n",
       "      <td>30-39</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Automóvel</td>\n",
       "      <td>15-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>571777</td>\n",
       "      <td>1269020</td>\n",
       "      <td>1018251</td>\n",
       "      <td>2024-01-01 01:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>ES</td>\n",
       "      <td>SERRA</td>\n",
       "      <td>DEL02-ES</td>\n",
       "      <td>-20.172928</td>\n",
       "      <td>-40.267364</td>\n",
       "      <td>Reação tardia ou ineficiente do condutor</td>\n",
       "      <td>Colisão com objeto</td>\n",
       "      <td>Plena Noite</td>\n",
       "      <td>Decrescente</td>\n",
       "      <td>Nublado</td>\n",
       "      <td>Múltipla</td>\n",
       "      <td>Interseção de Vias;Reta</td>\n",
       "      <td>urbano</td>\n",
       "      <td>Condutor</td>\n",
       "      <td>ileso</td>\n",
       "      <td>50-59</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>Caminhonete</td>\n",
       "      <td>15-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>571778</td>\n",
       "      <td>1269028</td>\n",
       "      <td>1018261</td>\n",
       "      <td>2024-01-01 00:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>SC</td>\n",
       "      <td>PENHA</td>\n",
       "      <td>DEL03-SC</td>\n",
       "      <td>-26.83477</td>\n",
       "      <td>-48.706151</td>\n",
       "      <td>Reação tardia ou ineficiente do condutor</td>\n",
       "      <td>Saída de leito carroçável</td>\n",
       "      <td>Plena Noite</td>\n",
       "      <td>Crescente</td>\n",
       "      <td>Chuva</td>\n",
       "      <td>Dupla</td>\n",
       "      <td>Curva</td>\n",
       "      <td>rural</td>\n",
       "      <td>Condutor</td>\n",
       "      <td>ileso</td>\n",
       "      <td>50-59</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>Camioneta</td>\n",
       "      <td>10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024</td>\n",
       "      <td>571778</td>\n",
       "      <td>1269045</td>\n",
       "      <td>1018261</td>\n",
       "      <td>2024-01-01 00:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>SC</td>\n",
       "      <td>PENHA</td>\n",
       "      <td>DEL03-SC</td>\n",
       "      <td>-26.83477</td>\n",
       "      <td>-48.706151</td>\n",
       "      <td>Reação tardia ou ineficiente do condutor</td>\n",
       "      <td>Saída de leito carroçável</td>\n",
       "      <td>Plena Noite</td>\n",
       "      <td>Crescente</td>\n",
       "      <td>Chuva</td>\n",
       "      <td>Dupla</td>\n",
       "      <td>Curva</td>\n",
       "      <td>rural</td>\n",
       "      <td>Passageiro</td>\n",
       "      <td>leve</td>\n",
       "      <td>30-39</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Camioneta</td>\n",
       "      <td>10-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ano_arquivo  sinistro_id  pessoa_id  veiculo_id           data_hora  \\\n",
       "0         2024       571772    1268971     1018215 2024-01-01 00:05:00   \n",
       "1         2024       571774    1268985     1018226 2024-01-01 00:05:00   \n",
       "3         2024       571777    1269020     1018251 2024-01-01 01:45:00   \n",
       "4         2024       571778    1269028     1018261 2024-01-01 00:45:00   \n",
       "5         2024       571778    1269045     1018261 2024-01-01 00:45:00   \n",
       "\n",
       "   dia_semana_num  uf municipio delegacia   latitude  longitude  \\\n",
       "0               0  RJ    TANGUA  DEL02-RJ  -22.72936 -42.701125   \n",
       "1               0  GO  ANAPOLIS  DEL02-GO -16.229185 -49.009797   \n",
       "3               0  ES     SERRA  DEL02-ES -20.172928 -40.267364   \n",
       "4               0  SC     PENHA  DEL03-SC  -26.83477 -48.706151   \n",
       "5               0  SC     PENHA  DEL03-SC  -26.83477 -48.706151   \n",
       "\n",
       "                             causa_acidente              tipo_acidente  \\\n",
       "0  Reação tardia ou ineficiente do condutor         Colisão com objeto   \n",
       "1                          Animais na Pista         Colisão com objeto   \n",
       "3  Reação tardia ou ineficiente do condutor         Colisão com objeto   \n",
       "4  Reação tardia ou ineficiente do condutor  Saída de leito carroçável   \n",
       "5  Reação tardia ou ineficiente do condutor  Saída de leito carroçável   \n",
       "\n",
       "      fase_dia  sentido_via condicao_meteorologica tipo_pista  \\\n",
       "0  Plena Noite  Decrescente              Céu Claro      Dupla   \n",
       "1  Plena Noite  Decrescente              Céu Claro      Dupla   \n",
       "3  Plena Noite  Decrescente                Nublado   Múltipla   \n",
       "4  Plena Noite    Crescente                  Chuva      Dupla   \n",
       "5  Plena Noite    Crescente                  Chuva      Dupla   \n",
       "\n",
       "               tracado_via caracteristicas_via tipo_envolvido estado_fisico  \\\n",
       "0                     Reta              urbano       Condutor         obito   \n",
       "1                     Reta               rural       Condutor         ileso   \n",
       "3  Interseção de Vias;Reta              urbano       Condutor         ileso   \n",
       "4                    Curva               rural       Condutor         ileso   \n",
       "5                    Curva               rural     Passageiro          leve   \n",
       "\n",
       "  faixa_etaria       sexo tipo_veiculo faixa_idade_veiculo  \n",
       "0        20-29  Masculino  Motocicleta                 0-4  \n",
       "1        30-39   Feminino    Automóvel               15-19  \n",
       "3        50-59  Masculino  Caminhonete               15-19  \n",
       "4        50-59  Masculino    Camioneta               10-14  \n",
       "5        30-39   Feminino    Camioneta               10-14  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def to_silver(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"\\nINICIANDO TRANSFORM (RAW -> SILVER)\")\n",
    "    print(f\"Shape inicial: {df.shape}\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalização geral\n",
    "    print(\"Normalizando texto...\")\n",
    "    for col in df.columns:\n",
    "        df[col] = normalize_text(df[col])\n",
    "\n",
    "    # Corrigir typo de coluna\n",
    "    if \"condicao_metereologica\" in df.columns:\n",
    "        df = df.rename(columns={\"condicao_metereologica\": \"condicao_meteorologica\"})\n",
    "\n",
    "    df[\"condicao_meteorologica\"] = null_if_unknown(df.get(\"condicao_meteorologica\"))\n",
    "\n",
    "    # IDs\n",
    "    print(\"Convertendo IDs...\")\n",
    "    df[\"sinistro_id\"] = to_int(df[\"id\"]) if \"id\" in df.columns else pd.NA\n",
    "    df[\"pessoa_id\"] = to_int(df[\"pesid\"]) if \"pesid\" in df.columns else pd.NA\n",
    "    df[\"veiculo_id\"] = to_int(df[\"id_veiculo\"]) if \"id_veiculo\" in df.columns else pd.NA\n",
    "    df[\"sinistro_id\"] = df[\"sinistro_id\"].where(df[\"sinistro_id\"].isna() | (df[\"sinistro_id\"] > 0), pd.NA)\n",
    "    df[\"pessoa_id\"]   = df[\"pessoa_id\"].where(df[\"pessoa_id\"].isna() | (df[\"pessoa_id\"] > 0), pd.NA)\n",
    "    df[\"veiculo_id\"]  = df[\"veiculo_id\"].where(df[\"veiculo_id\"].isna() | (df[\"veiculo_id\"] > 0), pd.NA)\n",
    "\n",
    "\n",
    "    # Data/hora\n",
    "    print(\"Convertendo data e horário...\")\n",
    "    df[\"date_dt\"] = pd.to_datetime(df.get(\"data_inversa\"), format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    df[\"time_dt\"] = parse_time(df.get(\"horario\"))\n",
    "\n",
    "    print(\"Criando data_hora...\")\n",
    "    time_txt = df[\"time_dt\"].astype(\"string\").fillna(\"00:00:00\")\n",
    "    df[\"data_hora\"] = pd.to_datetime(df[\"date_dt\"].astype(\"string\") + \" \" + time_txt, errors=\"coerce\")\n",
    "\n",
    "    df[\"ano_arquivo\"] = df[\"data_hora\"].dt.year.astype(\"Int64\")\n",
    "    df[\"dia_semana_num\"] = map_weekday(df.get(\"dia_semana\"))\n",
    "\n",
    "    # Coordenadas\n",
    "    print(\"Convertendo latitude/longitude...\")\n",
    "    df[\"latitude\"] = to_float(df.get(\"latitude\"))\n",
    "    df[\"longitude\"] = to_float(df.get(\"longitude\"))\n",
    "    df[\"latitude\"], df[\"longitude\"] = validate_coordinates(df[\"latitude\"], df[\"longitude\"])\n",
    "\n",
    "    # Campos derivados\n",
    "    df[\"caracteristicas_via\"] = map_land_use(df.get(\"uso_solo\"))\n",
    "    df[\"sexo_condutor\"] = map_gender(df.get(\"sexo\"))\n",
    "    df[\"estado_fisico\"] = map_physical_state(df.get(\"estado_fisico\"))\n",
    "    df[\"faixa_etaria\"] = age_bucket(df.get(\"idade\"))\n",
    "    df[\"faixa_idade_veiculo\"] = vehicle_age_bucket(df.get(\"ano_fabricacao_veiculo\"), df[\"ano_arquivo\"])\n",
    "\n",
    "    # UF + município\n",
    "    df[\"uf\"] = validate_uf(df.get(\"uf\"))\n",
    "    df[\"municipio\"] = df.get(\"municipio\").str.upper()\n",
    "\n",
    "    # Filtro de ano\n",
    "    print(\"Aplicando filtro ano_arquivo (2024/2025)...\")\n",
    "    before = len(df)\n",
    "    df = df[df[\"ano_arquivo\"].isin([2024, 2025])].copy()\n",
    "    print(f\"   Linhas removidas pelo filtro: {before - len(df):,}\")\n",
    "\n",
    "    # Garantir colunas finais\n",
    "    print(\"Garantindo colunas do contrato...\")\n",
    "    for col in SILVER_COLUMNS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    \n",
    "    before_pk = len(df)\n",
    "    df = df[df[\"sinistro_id\"].notna() & df[\"pessoa_id\"].notna()].copy()\n",
    "    print(f\"   Linhas removidas por PK inválida: {before_pk - len(df):,}\")\n",
    "\n",
    "\n",
    "    # Deduplicação por PK\n",
    "    print(\"Removendo duplicatas por (sinistro_id, pessoa_id)...\")\n",
    "    before = len(df)\n",
    "    df[\"__completeness\"] = df[SILVER_COLUMNS].notna().sum(axis=1)\n",
    "\n",
    "    df = df.sort_values(\n",
    "        [\"sinistro_id\", \"pessoa_id\", \"__completeness\", \"data_hora\"],\n",
    "        ascending=[True, True, False, False],\n",
    "        na_position=\"last\",\n",
    "    )\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"sinistro_id\", \"pessoa_id\"], keep=\"first\").drop(columns=\"__completeness\")\n",
    "    print(f\"   Duplicatas removidas: {before - len(df):,}\")\n",
    "\n",
    "    df_silver = df[SILVER_COLUMNS].copy()\n",
    "    print(f\"Shape final (silver): {df_silver.shape}\")\n",
    "    return df_silver\n",
    "\n",
    "df_silver = to_silver(df_raw)\n",
    "df_silver.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee807299",
   "metadata": {},
   "source": [
    "## 6. Validações e Exportação para CSV\n",
    "\n",
    "Antes de salvar, fazemos checagens simples para garantir qualidade:\n",
    "\n",
    "- distribuição por ano  \n",
    "- nulos nas chaves  \n",
    "- valores “ignorado” que deveriam ter virado NULL  \n",
    "\n",
    "Depois salvamos o CSV final na camada Silver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c542ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDAÇÕES\n",
      "Ano (value_counts):\n",
      "ano_arquivo\n",
      "2024    179114\n",
      "2025    160069\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Nulos nas chaves:\n",
      "sinistro_id null: 0\n",
      "pessoa_id   null: 0\n",
      "\n",
      "'ignorado' em condicao_meteorologica (ideal = 0):\n",
      "0\n",
      "\n",
      "CSV Silver salvo em: /Users/caiorocha/Workspace/unb/SBD2-Grupo-20-PRF/data_layer/silver/data/sinistros_silver.csv\n",
      "Tamanho (linhas): 339183\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVALIDAÇÕES\")\n",
    "\n",
    "print(\"Ano (value_counts):\")\n",
    "print(df_silver[\"ano_arquivo\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nNulos nas chaves:\")\n",
    "print(\"sinistro_id null:\", df_silver[\"sinistro_id\"].isna().sum())\n",
    "print(\"pessoa_id   null:\", df_silver[\"pessoa_id\"].isna().sum())\n",
    "\n",
    "print(\"\\n'ignorado' em condicao_meteorologica (ideal = 0):\")\n",
    "print((df_silver[\"condicao_meteorologica\"].astype(\"string\").str.lower() == \"ignorado\").sum())\n",
    "\n",
    "OUTPUT_FILE = SILVER_PATH / \"sinistros_silver.csv\"\n",
    "df_silver.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\nCSV Silver salvo em:\", OUTPUT_FILE)\n",
    "print(\"Tamanho (linhas):\", len(df_silver))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375570c8",
   "metadata": {},
   "source": [
    "## 7. Carga no PostgreSQL (silver.sinistros)\n",
    "\n",
    "Por fim, carregamos o dataset processado no banco PostgreSQL.  \n",
    "O modo `truncate` apaga tudo e recarrega do zero.  \n",
    "Depois validamos conferindo o total de registros na tabela.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023d47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CARREGANDO NO POSTGRES (silver.sinistros)...\n",
      "Total de linhas a serem carregadas: 339,183\n",
      "Estrutura do banco criada com sucesso\n",
      "Registros no banco antes da carga: 0\n",
      "⬆️  Inserindo dados...\n",
      "Carga concluída com sucesso!\n",
      "Registros no banco depois da carga: 339,183\n",
      "================================================================================\n",
      "\n",
      "Total no banco: 339,183\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCARREGANDO NO POSTGRES (silver.sinistros)...\")\n",
    "\n",
    "\n",
    "# Mode:\"truncate\" = limpa e recarrega tudo, \"upsert\"   = insere/atualiza se repetir PK\n",
    "load_to_postgres(df_silver, DB_CONFIG, mode=\"truncate\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
