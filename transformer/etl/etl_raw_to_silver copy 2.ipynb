{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a9b7c88",
   "metadata": {},
   "source": [
    "1. Importações e definições de variáveis de ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0bd2dbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. Configuração e Importações\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01municodedata\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "# 1. Configuração e Importações\n",
    "\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch\n",
    "import os\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 80)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ETL RAW -> SILVER | SINISTROS PRF\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10648602",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Caminhos (Raw e Silver)\n",
    "\n",
    "BASE_PATH = Path(os.getcwd()).parent.parent\n",
    "DATA_LAYER_RAW_PATH = BASE_PATH / \"data_layer\" / \"raw\"\n",
    "DATA_LAYER_SILVER_PATH = BASE_PATH / \"data_layer\" / \"silver\" / \"data\"\n",
    "\n",
    "DATA_LAYER_SILVER_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Procura automaticamente CSVs de 2024 e 2025 na pasta raw\n",
    "RAW_FILES = sorted([p for p in DATA_LAYER_RAW_PATH.iterdir() if p.suffix.lower() == \".csv\"])\n",
    "\n",
    "print(f\"Arquivos encontrados em raw: {len(RAW_FILES)}\")\n",
    "for p in RAW_FILES:\n",
    "    print(\" -\", p)\n",
    "\n",
    "# Se quiser fixar manualmente (opcional):\n",
    " RAW_FILES = [\n",
    "     DATA_LAYER_RAW_PATH / \"dados_brutos_2024.csv\",\n",
    "     DATA_LAYER_RAW_PATH / \"dados_brutos_2025.csv\",\n",
    " ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e628c107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Banco (env)\n",
    "\n",
    "load_dotenv(BASE_PATH / \".env\")\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"host\": os.getenv(\"DB_HOST\"),\n",
    "    \"port\": os.getenv(\"DB_PORT\"),\n",
    "    \"database\": os.getenv(\"POSTGRES_DB\"),\n",
    "    \"user\": os.getenv(\"POSTGRES_USER\"),\n",
    "    \"password\": os.getenv(\"POSTGRES_PASSWORD\"),\n",
    "}\n",
    "\n",
    "print(\"DB host:\", DB_CONFIG[\"host\"])\n",
    "print(\"DB port:\", DB_CONFIG[\"port\"])\n",
    "print(\"DB name:\", DB_CONFIG[\"database\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ff0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Normalização de texto (tirar espaços, padronizar nulos e evitar lixo)\n",
    "\n",
    "NULL_LIKE = {\n",
    "    \"\", \" \", \"null\", \"none\", \"nan\", \"na\", \"n/a\", \"(null)\", \"NoneType\", \"NaN\", \"NULL\", \"N/A\"\n",
    "}\n",
    "\n",
    "def normalize_text_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    - strip + colapsa espaços\n",
    "    - troca valores tipo 'null', 'nan', '' por <NA>\n",
    "    - mantém como string (nullable)\n",
    "    \"\"\"\n",
    "    s = s.astype(\"string\")\n",
    "\n",
    "    # normaliza unicode e remove espaços extras\n",
    "    s = s.map(lambda x: unicodedata.normalize(\"NFKC\", x) if pd.notna(x) else x)\n",
    "    s = s.str.strip()\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "    # padroniza nulos\n",
    "    s_lower = s.str.lower()\n",
    "    s = s.mask(s_lower.isin(NULL_LIKE), pd.NA)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def safe_to_int(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "\n",
    "def safe_to_float(s: pd.Series) -> pd.Series:\n",
    "    # aceita vírgula ou ponto\n",
    "    s = s.astype(\"string\").str.replace(\",\", \".\", regex=False)\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Float64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877943a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Parse de horário (aceita HH:MM:SS e HH:MM)\n",
    "\n",
    "def parse_time_series(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(\"string\")\n",
    "    s = normalize_text_series(s)\n",
    "\n",
    "    t1 = pd.to_datetime(s, format=\"%H:%M:%S\", errors=\"coerce\")\n",
    "    t2 = pd.to_datetime(s, format=\"%H:%M\", errors=\"coerce\")\n",
    "\n",
    "    t = t1.fillna(t2)\n",
    "    return t.dt.time  # python datetime.time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ad581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Dia da semana -> número (Seg=0 ... Dom=6)\n",
    "\n",
    "DIA_SEMANA_MAP = {\n",
    "    \"segunda-feira\": 0,\n",
    "    \"terca-feira\": 1,\n",
    "    \"terça-feira\": 1,\n",
    "    \"quarta-feira\": 2,\n",
    "    \"quinta-feira\": 3,\n",
    "    \"sexta-feira\": 4,\n",
    "    \"sabado\": 5,\n",
    "    \"sábado\": 5,\n",
    "    \"domingo\": 6,\n",
    "}\n",
    "\n",
    "def normalize_day_name(x: str) -> str:\n",
    "    if x is None or pd.isna(x):\n",
    "        return None\n",
    "    x = unicodedata.normalize(\"NFKD\", str(x)).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    x = x.strip().lower()\n",
    "    return x\n",
    "\n",
    "def map_dia_semana_num(s: pd.Series) -> pd.Series:\n",
    "    s = normalize_text_series(s)\n",
    "    s = s.map(normalize_day_name)\n",
    "    return s.map(DIA_SEMANA_MAP).astype(\"Int64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c542ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Padronizar sexo -> masculino | feminino | ignorado\n",
    "\n",
    "def padronizar_sexo(s: pd.Series) -> pd.Series:\n",
    "    s = normalize_text_series(s)\n",
    "\n",
    "    def _map(x):\n",
    "        if x is None or pd.isna(x):\n",
    "            return pd.NA\n",
    "        v = str(x).strip().lower()\n",
    "        v = unicodedata.normalize(\"NFKD\", v).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "\n",
    "        if v in {\"m\", \"masc\", \"masculino\"}:\n",
    "            return \"masculino\"\n",
    "        if v in {\"f\", \"fem\", \"feminino\"}:\n",
    "            return \"feminino\"\n",
    "\n",
    "        # tudo que não der pra confiar cai em ignorado\n",
    "        if v in {\"ignorado\", \"nao informado\", \"nao-informado\", \"não informado\", \"não-informado\", \"0\"}:\n",
    "            return \"ignorado\"\n",
    "\n",
    "        return \"ignorado\"\n",
    "\n",
    "    out = s.map(_map).astype(\"string\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c12f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 Faixa etária (idade_condutor / idade) -> bins 0-9 ... 100+\n",
    "\n",
    "def faixa_etaria_bins(idade_s: pd.Series) -> pd.Series:\n",
    "    idade = pd.to_numeric(idade_s, errors=\"coerce\")\n",
    "\n",
    "    # regra simples: 0 ou negativo = desconhecido\n",
    "    idade = idade.mask((idade <= 0) | (idade > 120), np.nan)\n",
    "\n",
    "    bins = [-0.1, 9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 10_000]\n",
    "    labels = [\"0-9\",\"10-19\",\"20-29\",\"30-39\",\"40-49\",\"50-59\",\"60-69\",\"70-79\",\"80-89\",\"90-99\",\"100+\"]\n",
    "\n",
    "    faixa = pd.cut(idade, bins=bins, labels=labels)\n",
    "    return faixa.astype(\"string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 Faixa de idade do veículo (ano_fabricacao_veiculo -> idade do veículo -> bins)\n",
    "\n",
    "def faixa_idade_veiculo_bins(ano_fab_s: pd.Series, ano_ref_s: pd.Series) -> pd.Series:\n",
    "    ano_fab = pd.to_numeric(ano_fab_s, errors=\"coerce\")\n",
    "    ano_ref = pd.to_numeric(ano_ref_s, errors=\"coerce\")\n",
    "\n",
    "    idade_veic = ano_ref - ano_fab\n",
    "    idade_veic = idade_veic.mask((idade_veic < 0) | (idade_veic > 120), np.nan)\n",
    "\n",
    "    bins = [-0.1, 4, 9, 14, 19, 29, 120]\n",
    "    labels = [\"0-4\",\"5-9\",\"10-14\",\"15-19\",\"20-29\",\"30+\"]\n",
    "\n",
    "    faixa = pd.cut(idade_veic, bins=bins, labels=labels)\n",
    "    return faixa.astype(\"string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f790f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_csvs(csv_paths: list[Path]) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    for p in csv_paths:\n",
    "        df = pd.read_csv(\n",
    "            p,\n",
    "            sep=\";\",\n",
    "            encoding=\"ISO-8859-1\",\n",
    "            low_memory=False,\n",
    "            dtype=str,  # lê tudo como string para controlar conversões depois\n",
    "        )\n",
    "        df[\"__source_file\"] = p.name\n",
    "        dfs.append(df)\n",
    "\n",
    "    df_all = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "    return df_all\n",
    "\n",
    "\n",
    "print(\"\\nCarregando dados Raw...\")\n",
    "df_raw = load_raw_csvs(RAW_FILES)\n",
    "\n",
    "print(f\"Carregado: {df_raw.shape[0]:,} linhas x {df_raw.shape[1]:,} colunas\")\n",
    "print(\"Colunas:\", list(df_raw.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737c0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Pipeline principal\n",
    "\n",
    "DDL_COLUMNS = [\n",
    "    \"ano_arquivo\",\n",
    "    \"sinistro_id\",\n",
    "    \"pessoa_id\",\n",
    "    \"veiculo_id\",\n",
    "    \"data_hora\",\n",
    "    \"dia_semana_num\",\n",
    "    \"uf\",\n",
    "    \"municipio\",\n",
    "    \"delegacia\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"causa_acidente\",\n",
    "    \"tipo_acidente\",\n",
    "    \"classificacao_acidente\",\n",
    "    \"fase_dia\",\n",
    "    \"sentido_via\",\n",
    "    \"condicao_meteorologica\",\n",
    "    \"tipo_pista\",\n",
    "    \"tracado_via\",\n",
    "    \"caracteristicas_via\",\n",
    "    \"tipo_envolvido\",\n",
    "    \"estado_fisico\",\n",
    "    \"faixa_etaria_condutor\",\n",
    "    \"sexo_condutor\",\n",
    "    \"tipo_veiculo\",\n",
    "    \"faixa_idade_veiculo\",\n",
    "    # \"created_at\" -> no banco tem default NOW(), então não precisa vir no insert\n",
    "]\n",
    "\n",
    "\n",
    "def transformar_para_silver(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"\\nINICIANDO TRANSFORM (RAW -> SILVER)\")\n",
    "    print(f\"Shape inicial: {df.shape}\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Normalizar strings em todas as colunas\n",
    "    print(\"1) Normalizando texto...\")\n",
    "    for col in df.columns:\n",
    "        df[col] = normalize_text_series(df[col])\n",
    "\n",
    "    # 2) Corrigir nome de coluna (condicao_metereologica -> condicao_meteorologica)\n",
    "    if \"condicao_metereologica\" in df.columns:\n",
    "        df = df.rename(columns={\"condicao_metereologica\": \"condicao_meteorologica\"})\n",
    "\n",
    "    # 3) IDs (padrão do banco)\n",
    "    print(\"2) Convertendo IDs...\")\n",
    "    if \"id\" in df.columns:\n",
    "        df[\"sinistro_id\"] = safe_to_int(df[\"id\"])\n",
    "    else:\n",
    "        df[\"sinistro_id\"] = pd.NA\n",
    "\n",
    "    if \"pesid\" in df.columns:\n",
    "        df[\"pessoa_id\"] = safe_to_int(df[\"pesid\"])\n",
    "    else:\n",
    "        df[\"pessoa_id\"] = pd.NA\n",
    "\n",
    "    if \"id_veiculo\" in df.columns:\n",
    "        df[\"veiculo_id\"] = safe_to_int(df[\"id_veiculo\"])\n",
    "    else:\n",
    "        df[\"veiculo_id\"] = pd.NA\n",
    "\n",
    "    # 4) Data e hora\n",
    "    print(\"3) Convertendo data e horário...\")\n",
    "    df[\"data_inversa_dt\"] = pd.to_datetime(df.get(\"data_inversa\"), format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    df[\"horario_time\"] = parse_time_series(df.get(\"horario\"))\n",
    "\n",
    "    # 5) data_hora (timestamp completo)\n",
    "    print(\"4) Criando data_hora...\")\n",
    "    df[\"data_hora\"] = pd.to_datetime(\n",
    "        df[\"data_inversa_dt\"].astype(\"string\") + \" \" + df[\"horario_time\"].astype(\"string\"),\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # 6) ano_arquivo (ano do sinistro)\n",
    "    df[\"ano_arquivo\"] = df[\"data_hora\"].dt.year.astype(\"Int64\")\n",
    "\n",
    "    # 7) dia_semana_num\n",
    "    df[\"dia_semana_num\"] = map_dia_semana_num(df.get(\"dia_semana\"))\n",
    "\n",
    "    # 8) Latitude / Longitude\n",
    "    print(\"5) Convertendo latitude/longitude...\")\n",
    "    df[\"latitude\"] = safe_to_float(df.get(\"latitude\"))\n",
    "    df[\"longitude\"] = safe_to_float(df.get(\"longitude\"))\n",
    "\n",
    "    # 9) caracteristicas_via (vem de uso_solo)\n",
    "    df[\"caracteristicas_via\"] = df.get(\"uso_solo\").astype(\"string\")\n",
    "\n",
    "    # 10) sexo_condutor (vem de sexo)\n",
    "    df[\"sexo_condutor\"] = padronizar_sexo(df.get(\"sexo\"))\n",
    "\n",
    "    # 11) faixa_etaria_condutor (vem de idade_condutor / idade)\n",
    "    # no seu CSV está como \"idade\"\n",
    "    df[\"faixa_etaria_condutor\"] = faixa_etaria_bins(df.get(\"idade\"))\n",
    "\n",
    "    # 12) faixa_idade_veiculo (derivada de ano_fabricacao_veiculo)\n",
    "    df[\"faixa_idade_veiculo\"] = faixa_idade_veiculo_bins(df.get(\"ano_fabricacao_veiculo\"), df[\"ano_arquivo\"])\n",
    "\n",
    "    # 13) UF em 2 letras maiúsculas\n",
    "    df[\"uf\"] = df.get(\"uf\").str.upper()\n",
    "\n",
    "    # 14) Filtro do DDL: apenas 2024 ou 2025\n",
    "    print(\"6) Aplicando filtro ano_arquivo (2024/2025)...\")\n",
    "    before = len(df)\n",
    "    df = df[df[\"ano_arquivo\"].isin([2024, 2025])].copy()\n",
    "    print(f\"   Linhas removidas pelo filtro: {before - len(df):,}\")\n",
    "\n",
    "    # 15) Ajustar colunas finais (DDL) + criar as que faltam como NULL\n",
    "    print(\"7) Garantindo colunas do DDL...\")\n",
    "    for col in DDL_COLUMNS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    # 16) Remover duplicatas na PK (sinistro_id, pessoa_id)\n",
    "    print(\"8) Removendo duplicatas por (sinistro_id, pessoa_id)...\")\n",
    "    before = len(df)\n",
    "    df = df.sort_values([\"sinistro_id\", \"pessoa_id\", \"data_hora\"], na_position=\"last\")\n",
    "    df = df.drop_duplicates(subset=[\"sinistro_id\", \"pessoa_id\"], keep=\"first\")\n",
    "    print(f\"   Duplicatas removidas: {before - len(df):,}\")\n",
    "\n",
    "    # 17) Selecionar só as colunas do contrato (DDL)\n",
    "    df_silver = df[DDL_COLUMNS].copy()\n",
    "\n",
    "    print(f\"Shape final (silver): {df_silver.shape}\")\n",
    "    return df_silver\n",
    "\n",
    "\n",
    "df_silver = transformar_para_silver(df_raw)\n",
    "df_silver.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f0088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVALIDACOES\")\n",
    "\n",
    "print(\"Ano (value_counts):\")\n",
    "print(df_silver[\"ano_arquivo\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nNulos nas chaves:\")\n",
    "print(\"sinistro_id null:\", df_silver[\"sinistro_id\"].isna().sum())\n",
    "print(\"pessoa_id   null:\", df_silver[\"pessoa_id\"].isna().sum())\n",
    "\n",
    "print(\"\\nExemplo de colunas:\")\n",
    "print(df_silver[[\"sinistro_id\",\"pessoa_id\",\"veiculo_id\",\"data_hora\",\"dia_semana_num\",\"sexo_condutor\",\"faixa_etaria_condutor\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c95ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = DATA_LAYER_SILVER_PATH / \"sinistros_silver.csv\"\n",
    "\n",
    "df_silver.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "print(\"CSV Silver salvo em:\", OUTPUT_FILE)\n",
    "print(\"Tamanho (linhas):\", len(df_silver))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
